{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lanza\\Integrated-vs-Seperated-Master-Thesis\\Data_Test_Multi_Raw\n",
      "   day  month  year  is_holiday    location  temperature\n",
      "0   10      5  2022       False       Rehau         7.74\n",
      "1   20      9  2020       False  Holzminden         9.37\n",
      "2   12      5  2020       False    Grafenau         8.97\n",
      "3    1      4  2020       False     Parchim        10.05\n",
      "4   23      8  2022       False     Ansbach         7.32\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = os.path.join(os.path.dirname(os.getcwd()), 'Data_Test_Multi_Raw')\n",
    "print(folder_path)\n",
    "file_names = ['data_test.csv', 'data_train.csv', 'target_test.csv', 'target_train.csv']\n",
    "\n",
    "data_frames = []\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    df = pd.read_csv(file_path)\n",
    "    data_frames.append(df)\n",
    "\n",
    "data_test = data_frames[0]\n",
    "data_train = data_frames[1]\n",
    "target_test = data_frames[2]    \n",
    "target_train = data_frames[3]\n",
    "\n",
    "print(data_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize an empty list to store the final order quantities\n",
    "final_order_quantities_ANN = []\n",
    "final_order_quantities_DT = []\n",
    "\n",
    "# Parameters for multi-item newsvendor problem\n",
    "prices = np.array([0.3, 0.5, 0.6, 0.5, 0.5, 0.5]) #price data\n",
    "costs = np.array([0.06, 0.06, 0.06, 0.06, 0.06, 0.06]) #cost data\n",
    "salvages = np.array([0.01, 0.01, 0.01, 0.01, 0.01, 0.01]) #salvage data\n",
    "underage_data = prices - costs \n",
    "overage_data = costs - salvages \n",
    "\n",
    "\n",
    "alpha_data = np.array([             #alpha data\n",
    "    [0.0, 0.1, 0.05, 0.1, 0.05, 0.1],\n",
    "    [0.15, 0.0, 0.1, 0.05, 0.05, 0.05],\n",
    "    [0.1, 0.2, 0.0, 0.05, 0.1, 0.05],\n",
    "    [0.05, 0.05, 0.05, 0.0, 0.15, 0.2],\n",
    "    [0.1, 0.05, 0.15, 0.2, 0.0, 0.05],\n",
    "    [0.05, 0.1, 0.05, 0.15, 0.1, 0.0]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.2614810567465224 0.0 0.0 ... 5 2022 False]\n",
      " [0.03038460101498815 0.0 0.0 ... 9 2020 False]\n",
      " [-0.2866376462884491 0.0 0.0 ... 5 2020 False]\n",
      " ...\n",
      " [-1.2614810567465224 0.0 0.0 ... 1 2021 False]\n",
      " [-1.0316399274515304 0.0 0.0 ... 1 2022 False]\n",
      " [0.03038460101498815 0.0 0.0 ... 4 2020 True]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define preprocessing for numeric columns (scale them)\n",
    "numeric_features = ['temperature']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Define preprocessing for categorical features (encode them)\n",
    "categorical_features = ['location']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)],\n",
    "    remainder='passthrough')\n",
    "\n",
    "# Preprocessing on train data\n",
    "X_train = preprocessor.fit_transform(data_train)\n",
    "\n",
    "# Preprocessing on test data\n",
    "X_test = preprocessor.transform(data_test)\n",
    "\n",
    "\n",
    "print(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import KFold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_NN_model(X_train, y_train, X_val, y_val, alpha_input, underage_input, overage_input, patience=10, multi=True, integrated=True, verbose=0, seed=42):\n",
    "\n",
    "    \"\"\" Train a network on the given training data with hyperparameter tuning\n",
    "    \n",
    "    Parameters\n",
    "    --------------\n",
    "    X_train : np.array\n",
    "        training feature data\n",
    "    y_train : np.array\n",
    "        training targets\n",
    "    X_train : np.array\n",
    "        validation feature data\n",
    "    y_train : np.array\n",
    "        validation targets\n",
    "    alpha : np.array\n",
    "        Substitution rates, shape (N_PRODUCTS, N_PRODUCTS)\n",
    "    u : np.array\n",
    "        underage costs, shape (1, N_PRODUCTS)\n",
    "    o : np.array\n",
    "        overage costs, shape (1, N_PRODUCTS)\n",
    "    patience : int\n",
    "        number of epochs without improvement before stopping training\n",
    "    verbose : int\n",
    "        keras' verbose parameter for silent / verbose model training\n",
    "    seed : int\n",
    "        random seed (affects mainly model initialization, set for reproducable results)\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    model : keras model\n",
    "        Final model\n",
    "    hp : list or tupl\n",
    "        hyperparameters in the following order: hidden_nodes, lr, max_epochs, patience, batch_size\n",
    "    val_profit : float\n",
    "        Mean profit on the validation set\n",
    "    \"\"\"\n",
    "    global alpha, underage, overage\n",
    "    alpha = alpha_input\n",
    "    underage = underage_input\n",
    "    overage = overage_input\n",
    "\n",
    "    output_shape = y_train.shape[1]\n",
    "    input_shape = X_train.shape[1]\n",
    "\n",
    "    # create a neural network model with basic hyperparameters\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "\n",
    "    # construct loss function based on the number of products\n",
    "    if not integrated:\n",
    "        model_ANN = KerasRegressor(model=create_NN_basic, n_hidden=1,n_neurons=30, activation = 'relu',\n",
    "                                input_shape=input_shape, learning_rate=0.01, output_shape=output_shape, \n",
    "                                seed = seed, verbose=verbose, callbacks=[early_stopping])\n",
    "    elif not multi and integrated:\n",
    "        model_ANN = KerasRegressor(model=create_NN_single, n_hidden=1,n_neurons=30, activation = 'relu',\n",
    "                               input_shape=input_shape, learning_rate=0.01, output_shape=output_shape, \n",
    "                               seed = seed, verbose=verbose, callbacks=[early_stopping])\n",
    "\n",
    "    elif multi and integrated: \n",
    "        model_ANN = KerasRegressor(model=create_NN_multi, n_hidden=1,n_neurons=30, activation = 'relu',\n",
    "                               input_shape=input_shape, learning_rate=0.01, output_shape=output_shape, \n",
    "                               seed = seed, verbose=verbose, callbacks=[early_stopping])\n",
    "    else:\n",
    "        raise ValueError('Invalid Configuration')\n",
    "    \n",
    "\n",
    "    \n",
    "    # define the hyperparameters space\n",
    "    param_distribs = {\n",
    "        \"n_hidden\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        \"n_neurons\": np.arange(1, 30),\n",
    "        \"learning_rate\": [0.01,0.001,0.0001,0.00001],\n",
    "        \"batch_size\": [16, 32, 64, 128],\n",
    "        \"epochs\": [10, 20, 30, 40, 50],\n",
    "        \"activation\": ['relu', 'sigmoid', 'tanh']\n",
    "    }\n",
    "\n",
    "    # perform GridSearch for hyperparameter tuning\n",
    "    random_CV = RandomizedSearchCV(model_ANN, param_distribs, n_iter=100, cv=3, scoring='neg_mean_squared_error')\n",
    "    random_CV_result = random_CV.fit(X_train, y_train, validation_data=(X_val, y_val), verbose=verbose)\n",
    "\n",
    "    # Get the best parameters and best estimator\n",
    "    best_params = random_CV_result.best_params_\n",
    "    best_estimator = random_CV_result.best_estimator_\n",
    "\n",
    "    # make predictions on validation set and compute profits\n",
    "    q_val = best_estimator.predict(X_val)\n",
    "    val_profit = np.mean(nvps_profit(y_val, q_val, alpha, underage, overage))\n",
    "\n",
    "    hyperparameter = [best_params['n_hidden'], best_params['n_neurons'],best_params['learning_rate'], \n",
    "                      best_params['epochs'], patience, best_params['batch_size'], best_params['activation']]\n",
    "\n",
    "    return best_estimator, hyperparameter, val_profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nvps_loss(alpha, u, o):\n",
    "\n",
    "    # transofrm the alpha, u, o to tensors\n",
    "    u = tf.convert_to_tensor(underage_data, dtype=tf.float32) #underage costs\n",
    "    o = tf.convert_to_tensor(overage_data, dtype=tf.float32) #overage costs\n",
    "    alpha = tf.convert_to_tensor(alpha_data, dtype=tf.float32) #substitution matrix\n",
    "\n",
    "    # define the loss function\n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def nvps_loss(y_true, y_pred):\n",
    "        q = tf.maximum(y_pred, 0.)\n",
    "\n",
    "        # Calculate the demand increase for each product due to substitutions from other products\n",
    "        demand_increase = tf.matmul( tf.maximum(0.0, y_true - y_pred),alpha)\n",
    "        # Adjusted demand is the original demand plus the increase due to substitutions\n",
    "        adjusted_demand = y_true + demand_increase\n",
    "\n",
    "        profits = tf.matmul(q,tf.transpose(u)) - tf.matmul(tf.maximum(0.0,q - adjusted_demand), tf.transpose(u+o))\n",
    "\n",
    "        return -tf.math.reduce_mean(profits)\n",
    "    \n",
    "    return nvps_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lanza\\Integrated-vs-Seperated-Master-Thesis\\.venv\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Users\\lanza\\Integrated-vs-Seperated-Master-Thesis\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not interpret metric identifier: loss",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 65\u001b[0m\n\u001b[0;32m     49\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_hidden\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m15\u001b[39m),\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_neurons\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     56\u001b[0m }\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Optimize the model using RandomizedSearchCV\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m#rnd_search_cv_ANN = RandomizedSearchCV(model_ANN, param_distributions=params, cv=KFold(10))\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m#rnd_search_cv_ANN.fit(X_train, target_train)\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m#print(rnd_search_cv_ANN.best_params_)\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m \u001b[43mmodel_ANN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m target_pred_ANN \u001b[38;5;241m=\u001b[39m model_ANN\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\lanza\\Integrated-vs-Seperated-Master-Thesis\\.venv\\Lib\\site-packages\\scikeras\\wrappers.py:760\u001b[0m, in \u001b[0;36mBaseWrapper.fit\u001b[1;34m(self, X, y, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    755\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    756\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit__epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs)\n\u001b[0;32m    757\u001b[0m )\n\u001b[0;32m    758\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 760\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    763\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    764\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarm_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarm_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    765\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    766\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\lanza\\Integrated-vs-Seperated-Master-Thesis\\.venv\\Lib\\site-packages\\scikeras\\wrappers.py:928\u001b[0m, in \u001b[0;36mBaseWrapper._fit\u001b[1;34m(self, X, y, sample_weight, warm_start, epochs, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    924\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_encoder_\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_model_compatibility(y)\n\u001b[1;32m--> 928\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_keras_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarm_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lanza\\Integrated-vs-Seperated-Master-Thesis\\.venv\\Lib\\site-packages\\scikeras\\wrappers.py:536\u001b[0m, in \u001b[0;36mBaseWrapper._fit_keras_model\u001b[1;34m(self, X, y, sample_weight, warm_start, epochs, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;66;03m# Keras puts keys like \"val_accuracy\" and \"loss\" and\u001b[39;00m\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;66;03m# \"val_loss\" in hist.history\u001b[39;00m\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown metric function\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n\u001b[1;32m--> 536\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory_[key] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[1;32mc:\\Users\\lanza\\Integrated-vs-Seperated-Master-Thesis\\.venv\\Lib\\site-packages\\scikeras\\wrappers.py:531\u001b[0m, in \u001b[0;36mBaseWrapper._fit_keras_model\u001b[1;34m(self, X, y, sample_weight, warm_start, epochs, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m hist\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 531\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[43mmetric_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;66;03m# Keras puts keys like \"val_accuracy\" and \"loss\" and\u001b[39;00m\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;66;03m# \"val_loss\" in hist.history\u001b[39;00m\n\u001b[0;32m    535\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown metric function\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[1;32mc:\\Users\\lanza\\Integrated-vs-Seperated-Master-Thesis\\.venv\\Lib\\site-packages\\scikeras\\utils\\__init__.py:111\u001b[0m, in \u001b[0;36mmetric_name\u001b[1;34m(metric)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(metric, (\u001b[38;5;28mstr\u001b[39m, Metric)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(metric)):\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m``metric`` must be a string, a function, an instance of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ``tf.keras.metrics.Metric`` or a type inheriting from\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ``tf.keras.metrics.Metric``\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m     )\n\u001b[1;32m--> 111\u001b[0m fn_or_cls \u001b[38;5;241m=\u001b[39m \u001b[43mkeras_metric_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fn_or_cls, Metric):\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _camel2snake(fn_or_cls\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lanza\\Integrated-vs-Seperated-Master-Thesis\\.venv\\Lib\\site-packages\\keras\\src\\metrics\\__init__.py:206\u001b[0m, in \u001b[0;36mget\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not interpret metric identifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Could not interpret metric identifier: loss"
     ]
    }
   ],
   "source": [
    "def loss_complex(y_true, y_pred):\n",
    "\n",
    "    # Cast numpy arrays to tensors\n",
    "    u = tf.convert_to_tensor(underage_data, dtype=tf.float32) #underage costs\n",
    "    o = tf.convert_to_tensor(overage_data, dtype=tf.float32) #overage costs\n",
    "    alpha = tf.convert_to_tensor(alpha_data, dtype=tf.float32) #substitution matrix\n",
    "\n",
    "    # Cast y_true to float32\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "\n",
    "    # Calculate the demand increase for each product due to substitutions from other products\n",
    "    demand_increase = tf.matmul( tf.maximum(0.0, y_true - y_pred),alpha)\n",
    "\n",
    "    # Adjusted demand is the original demand plus the increase due to substitutions\n",
    "    adjusted_demand = y_true + demand_increase\n",
    "\n",
    "    # Compute the loss with adjusted demand\n",
    "    loss = -tf.reduce_mean(u * y_pred - (u + o) * tf.maximum(y_pred - adjusted_demand, 0))\n",
    "    return loss\n",
    "\n",
    "from keras.utils import get_custom_objects\n",
    "get_custom_objects().update({'loss_complex': loss_complex})\n",
    "\n",
    "# Model creation function \n",
    "def create_model(n_hidden, n_neurons, learning_rate, activation):\n",
    "    model = Sequential()\n",
    "    # Input Layer\n",
    "    model.add(Dense(n_neurons,input_dim=15, activation=activation))\n",
    "    # Hidden Layer\n",
    "    for _ in range(n_hidden):\n",
    "        model.add(Dense(n_neurons, activation=activation))\n",
    "    # Output Layer\n",
    "    model.add(Dense(6))\n",
    "    model.compile(loss = loss_complex,\n",
    "                  optimizer=Adam(learning_rate=learning_rate))\n",
    "    return model\n",
    "\n",
    "# Model builder function \n",
    "def model_builder(n_hidden, n_neurons, learning_rate, activation, batch_size, epochs):\n",
    "    return KerasRegressor(model=create_model, verbose=0, n_hidden=n_hidden, n_neurons=n_neurons, \n",
    "                          learning_rate=learning_rate, activation=activation, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# Create a baseline model\n",
    "model_ANN = model_builder(11,27,0.00014593446517329207,'relu', 16, 25)\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "from scipy.stats import reciprocal\n",
    "params = {\n",
    "    \"n_hidden\": range(0, 15),\n",
    "    \"n_neurons\": np.arange(1, 100),\n",
    "    \"learning_rate\": reciprocal(1e-4, 1e-2),\n",
    "    \"batch_size\": [16, 32, 64, 128],\n",
    "    \"epochs\": [10,15, 20, 25, 30],\n",
    "    \"activation\": ['relu', 'sigmoid', 'tanh'] #\n",
    "}\n",
    "\n",
    "# Optimize the model using RandomizedSearchCV\n",
    "#rnd_search_cv_ANN = RandomizedSearchCV(model_ANN, param_distributions=params, cv=KFold(10))\n",
    "\n",
    "# Fit the model\n",
    "#rnd_search_cv_ANN.fit(X_train, target_train)\n",
    "#print(rnd_search_cv_ANN.best_params_)\n",
    "\n",
    "model_ANN.fit(X_train, target_train)\n",
    "target_pred_ANN = model_ANN.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lanza\\Integrated-vs-Seperated-Master-Thesis\\.venv\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasRegressor(\n",
      "\tmodel=None\n",
      "\tbuild_fn=<function create_model at 0x000001C0FC33F920>\n",
      "\twarm_start=False\n",
      "\trandom_state=None\n",
      "\toptimizer=rmsprop\n",
      "\tloss=None\n",
      "\tmetrics=None\n",
      "\tbatch_size=16\n",
      "\tvalidation_batch_size=None\n",
      "\tverbose=0\n",
      "\tcallbacks=None\n",
      "\tvalidation_split=0.0\n",
      "\tshuffle=True\n",
      "\trun_eagerly=False\n",
      "\tepochs=25\n",
      "\tn_hidden=11\n",
      "\tn_neurons=27\n",
      "\tlearning_rate=0.00014593446517329207\n",
      "\tactivation=relu\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Predict with best model\n",
    "best_model_ANN = rnd_search_cv_ANN.best_estimator_\n",
    "best_model_ANN.fit(X_train, target_train)\n",
    "target_pred_ANN = best_model_ANN.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Model Tuning\n",
      "(800, 15)\n",
      "(800, 6)\n",
      "(6, 6)\n",
      "(6,)\n",
      "(6,)\n",
      "XGB Model Tuning\n",
      "[0]\tTrain-rmse:46.36803\tTrain-newsvendorRMSE:4568.74343\n",
      "[1]\tTrain-rmse:38.24588\tTrain-newsvendorRMSE:3737.27768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\tTrain-rmse:31.73315\tTrain-newsvendorRMSE:3067.80059\n",
      "[3]\tTrain-rmse:26.50656\tTrain-newsvendorRMSE:2528.40375\n",
      "[4]\tTrain-rmse:22.28268\tTrain-newsvendorRMSE:2090.94756\n",
      "[5]\tTrain-rmse:18.88367\tTrain-newsvendorRMSE:1740.60578\n",
      "[6]\tTrain-rmse:16.13330\tTrain-newsvendorRMSE:1462.56776\n",
      "[7]\tTrain-rmse:13.89224\tTrain-newsvendorRMSE:1237.33341\n",
      "[8]\tTrain-rmse:12.04993\tTrain-newsvendorRMSE:1052.27368\n",
      "[9]\tTrain-rmse:10.52957\tTrain-newsvendorRMSE:900.21597\n",
      "[10]\tTrain-rmse:9.26887\tTrain-newsvendorRMSE:775.09057\n",
      "[11]\tTrain-rmse:8.21662\tTrain-newsvendorRMSE:672.46042\n",
      "[12]\tTrain-rmse:7.33108\tTrain-newsvendorRMSE:588.01726\n",
      "[13]\tTrain-rmse:6.58464\tTrain-newsvendorRMSE:518.13865\n",
      "[14]\tTrain-rmse:5.95129\tTrain-newsvendorRMSE:459.30921\n",
      "[15]\tTrain-rmse:5.41336\tTrain-newsvendorRMSE:409.78031\n",
      "[16]\tTrain-rmse:4.95069\tTrain-newsvendorRMSE:367.59584\n",
      "[17]\tTrain-rmse:4.55185\tTrain-newsvendorRMSE:331.74497\n",
      "[18]\tTrain-rmse:4.20692\tTrain-newsvendorRMSE:301.26788\n",
      "[19]\tTrain-rmse:3.90666\tTrain-newsvendorRMSE:275.23653\n",
      "[20]\tTrain-rmse:3.64417\tTrain-newsvendorRMSE:253.00945\n",
      "[21]\tTrain-rmse:3.41331\tTrain-newsvendorRMSE:234.00327\n",
      "[22]\tTrain-rmse:3.20895\tTrain-newsvendorRMSE:217.77426\n",
      "[23]\tTrain-rmse:3.02693\tTrain-newsvendorRMSE:203.72014\n",
      "[24]\tTrain-rmse:2.86414\tTrain-newsvendorRMSE:191.45452\n",
      "[25]\tTrain-rmse:2.71787\tTrain-newsvendorRMSE:180.70821\n",
      "[26]\tTrain-rmse:2.58589\tTrain-newsvendorRMSE:171.21892\n",
      "[27]\tTrain-rmse:2.46635\tTrain-newsvendorRMSE:162.80944\n",
      "[28]\tTrain-rmse:2.35764\tTrain-newsvendorRMSE:155.30018\n",
      "[29]\tTrain-rmse:2.25859\tTrain-newsvendorRMSE:148.57022\n",
      "[30]\tTrain-rmse:2.16806\tTrain-newsvendorRMSE:142.48802\n",
      "[31]\tTrain-rmse:2.08525\tTrain-newsvendorRMSE:136.95588\n",
      "[32]\tTrain-rmse:2.00926\tTrain-newsvendorRMSE:131.94788\n",
      "[33]\tTrain-rmse:1.93921\tTrain-newsvendorRMSE:127.35764\n",
      "[34]\tTrain-rmse:1.87475\tTrain-newsvendorRMSE:123.17647\n",
      "[35]\tTrain-rmse:1.81531\tTrain-newsvendorRMSE:119.36249\n",
      "[36]\tTrain-rmse:1.76016\tTrain-newsvendorRMSE:115.82930\n",
      "[37]\tTrain-rmse:1.70906\tTrain-newsvendorRMSE:112.56666\n",
      "[38]\tTrain-rmse:1.66164\tTrain-newsvendorRMSE:109.57225\n",
      "[39]\tTrain-rmse:1.61742\tTrain-newsvendorRMSE:106.78546\n",
      "[40]\tTrain-rmse:1.57623\tTrain-newsvendorRMSE:104.20741\n",
      "[41]\tTrain-rmse:1.53764\tTrain-newsvendorRMSE:101.80424\n",
      "[42]\tTrain-rmse:1.50146\tTrain-newsvendorRMSE:99.56129\n",
      "[43]\tTrain-rmse:1.46745\tTrain-newsvendorRMSE:97.46162\n",
      "[44]\tTrain-rmse:1.43535\tTrain-newsvendorRMSE:95.48515\n",
      "[45]\tTrain-rmse:1.40501\tTrain-newsvendorRMSE:93.61924\n",
      "[46]\tTrain-rmse:1.37624\tTrain-newsvendorRMSE:91.85314\n",
      "[47]\tTrain-rmse:1.34889\tTrain-newsvendorRMSE:90.17423\n",
      "[48]\tTrain-rmse:1.32279\tTrain-newsvendorRMSE:88.57192\n",
      "[49]\tTrain-rmse:1.29790\tTrain-newsvendorRMSE:87.04153\n",
      "[50]\tTrain-rmse:1.27401\tTrain-newsvendorRMSE:85.57165\n",
      "[51]\tTrain-rmse:1.25104\tTrain-newsvendorRMSE:84.15477\n",
      "[52]\tTrain-rmse:1.22899\tTrain-newsvendorRMSE:82.79147\n",
      "[53]\tTrain-rmse:1.20771\tTrain-newsvendorRMSE:81.47259\n",
      "[54]\tTrain-rmse:1.18717\tTrain-newsvendorRMSE:80.19476\n",
      "[55]\tTrain-rmse:1.16728\tTrain-newsvendorRMSE:78.95304\n",
      "[56]\tTrain-rmse:1.14799\tTrain-newsvendorRMSE:77.74473\n",
      "[57]\tTrain-rmse:1.12927\tTrain-newsvendorRMSE:76.56697\n",
      "[58]\tTrain-rmse:1.11106\tTrain-newsvendorRMSE:75.41768\n",
      "[59]\tTrain-rmse:1.09333\tTrain-newsvendorRMSE:74.29442\n",
      "[60]\tTrain-rmse:1.07605\tTrain-newsvendorRMSE:73.19523\n",
      "[61]\tTrain-rmse:1.05916\tTrain-newsvendorRMSE:72.11780\n",
      "[62]\tTrain-rmse:1.04268\tTrain-newsvendorRMSE:71.06183\n",
      "[63]\tTrain-rmse:1.02656\tTrain-newsvendorRMSE:70.02521\n",
      "[64]\tTrain-rmse:1.01076\tTrain-newsvendorRMSE:69.00563\n",
      "[65]\tTrain-rmse:0.99529\tTrain-newsvendorRMSE:68.00399\n",
      "[66]\tTrain-rmse:0.98014\tTrain-newsvendorRMSE:67.01943\n",
      "[67]\tTrain-rmse:0.96526\tTrain-newsvendorRMSE:66.04950\n",
      "[68]\tTrain-rmse:0.95067\tTrain-newsvendorRMSE:65.09569\n",
      "[69]\tTrain-rmse:0.93635\tTrain-newsvendorRMSE:64.15601\n",
      "[70]\tTrain-rmse:0.92229\tTrain-newsvendorRMSE:63.23063\n",
      "[71]\tTrain-rmse:0.90849\tTrain-newsvendorRMSE:62.32059\n",
      "[72]\tTrain-rmse:0.89492\tTrain-newsvendorRMSE:61.42274\n",
      "[73]\tTrain-rmse:0.88160\tTrain-newsvendorRMSE:60.53957\n",
      "[74]\tTrain-rmse:0.86848\tTrain-newsvendorRMSE:59.66749\n",
      "[75]\tTrain-rmse:0.85559\tTrain-newsvendorRMSE:58.80822\n",
      "[76]\tTrain-rmse:0.84289\tTrain-newsvendorRMSE:57.96083\n",
      "[77]\tTrain-rmse:0.83040\tTrain-newsvendorRMSE:57.12507\n",
      "[78]\tTrain-rmse:0.81809\tTrain-newsvendorRMSE:56.29993\n",
      "[79]\tTrain-rmse:0.80597\tTrain-newsvendorRMSE:55.48668\n",
      "[80]\tTrain-rmse:0.79405\tTrain-newsvendorRMSE:54.68472\n",
      "[81]\tTrain-rmse:0.78233\tTrain-newsvendorRMSE:53.89501\n",
      "[82]\tTrain-rmse:0.77077\tTrain-newsvendorRMSE:53.11507\n",
      "[83]\tTrain-rmse:0.75941\tTrain-newsvendorRMSE:52.34757\n",
      "[84]\tTrain-rmse:0.74821\tTrain-newsvendorRMSE:51.58938\n",
      "[85]\tTrain-rmse:0.73721\tTrain-newsvendorRMSE:50.84398\n",
      "[86]\tTrain-rmse:0.72636\tTrain-newsvendorRMSE:50.10802\n",
      "[87]\tTrain-rmse:0.71566\tTrain-newsvendorRMSE:49.38072\n",
      "[88]\tTrain-rmse:0.70515\tTrain-newsvendorRMSE:48.66605\n",
      "[89]\tTrain-rmse:0.69476\tTrain-newsvendorRMSE:47.95868\n",
      "[90]\tTrain-rmse:0.68455\tTrain-newsvendorRMSE:47.26240\n",
      "[91]\tTrain-rmse:0.67449\tTrain-newsvendorRMSE:46.57600\n",
      "[92]\tTrain-rmse:0.66460\tTrain-newsvendorRMSE:45.90077\n",
      "[93]\tTrain-rmse:0.65484\tTrain-newsvendorRMSE:45.23408\n",
      "[94]\tTrain-rmse:0.64524\tTrain-newsvendorRMSE:44.57723\n",
      "[95]\tTrain-rmse:0.63574\tTrain-newsvendorRMSE:43.92686\n",
      "[96]\tTrain-rmse:0.62642\tTrain-newsvendorRMSE:43.28750\n",
      "[97]\tTrain-rmse:0.61724\tTrain-newsvendorRMSE:42.65792\n",
      "[98]\tTrain-rmse:0.60816\tTrain-newsvendorRMSE:42.03487\n",
      "[99]\tTrain-rmse:0.59925\tTrain-newsvendorRMSE:41.42276\n",
      "[100]\tTrain-rmse:0.59046\tTrain-newsvendorRMSE:40.81949\n",
      "[101]\tTrain-rmse:0.58178\tTrain-newsvendorRMSE:40.22306\n",
      "[102]\tTrain-rmse:0.57326\tTrain-newsvendorRMSE:39.63714\n",
      "[103]\tTrain-rmse:0.56486\tTrain-newsvendorRMSE:39.05902\n",
      "[104]\tTrain-rmse:0.55658\tTrain-newsvendorRMSE:38.48984\n",
      "[105]\tTrain-rmse:0.54842\tTrain-newsvendorRMSE:37.92804\n",
      "[106]\tTrain-rmse:0.54038\tTrain-newsvendorRMSE:37.37480\n",
      "[107]\tTrain-rmse:0.53245\tTrain-newsvendorRMSE:36.82841\n",
      "[108]\tTrain-rmse:0.52465\tTrain-newsvendorRMSE:36.29130\n",
      "[109]\tTrain-rmse:0.51698\tTrain-newsvendorRMSE:35.76244\n",
      "[110]\tTrain-rmse:0.50941\tTrain-newsvendorRMSE:35.24046\n",
      "[111]\tTrain-rmse:0.50195\tTrain-newsvendorRMSE:34.72651\n",
      "[112]\tTrain-rmse:0.49459\tTrain-newsvendorRMSE:34.21884\n",
      "[113]\tTrain-rmse:0.48736\tTrain-newsvendorRMSE:33.72028\n",
      "[114]\tTrain-rmse:0.48023\tTrain-newsvendorRMSE:33.22866\n",
      "[115]\tTrain-rmse:0.47321\tTrain-newsvendorRMSE:32.74406\n",
      "[116]\tTrain-rmse:0.46629\tTrain-newsvendorRMSE:32.26646\n",
      "[117]\tTrain-rmse:0.45948\tTrain-newsvendorRMSE:31.79614\n",
      "[118]\tTrain-rmse:0.45275\tTrain-newsvendorRMSE:31.33157\n",
      "[119]\tTrain-rmse:0.44614\tTrain-newsvendorRMSE:30.87532\n",
      "[120]\tTrain-rmse:0.43961\tTrain-newsvendorRMSE:30.42459\n",
      "[121]\tTrain-rmse:0.43319\tTrain-newsvendorRMSE:29.98098\n",
      "[122]\tTrain-rmse:0.42686\tTrain-newsvendorRMSE:29.54345\n",
      "[123]\tTrain-rmse:0.42059\tTrain-newsvendorRMSE:29.11094\n",
      "[124]\tTrain-rmse:0.41443\tTrain-newsvendorRMSE:28.68506\n",
      "[125]\tTrain-rmse:0.40836\tTrain-newsvendorRMSE:28.26522\n",
      "[126]\tTrain-rmse:0.40237\tTrain-newsvendorRMSE:27.85191\n",
      "[127]\tTrain-rmse:0.39651\tTrain-newsvendorRMSE:27.44622\n",
      "Predictions: [[53.97771  67.13972  57.313854 31.094997 75.023544 77.08146 ]\n",
      " [58.996433 73.13882  63.2913   36.022488 81.028755 84.077965]\n",
      " [48.996243 61.110188 52.2873   28.935966 68.023384 70.06484 ]\n",
      " ...\n",
      " [53.97771  67.13972  57.313854 31.094997 75.023544 77.08146 ]\n",
      " [54.995266 69.10332  59.22984  32.07876  77.022575 79.06334 ]\n",
      " [69.821106 85.78061  75.173355 45.62614  94.11513  97.4986  ]]\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "print(\"XGB Model Tuning\")\n",
    "print(X_train.shape)\n",
    "print(target_train.shape)\n",
    "print(alpha_data.shape)\n",
    "print(underage_data.shape)\n",
    "print(overage_data.shape)\n",
    "print(\"XGB Model Tuning\")\n",
    "\n",
    "def custom_XGB_model():\n",
    "    \n",
    "    def gradient(predt: np.ndarray, dtrain: xgb.DMatrix) -> np.ndarray:\n",
    "        \n",
    "        y = dtrain.get_label().reshape(predt.shape)\n",
    "        d = y + np.matmul(np.maximum(0, y - predt), alpha_data)\n",
    "        u = np.array(underage_data)\n",
    "        o = np.array(overage_data)\n",
    "        return (-(u * np.maximum(0,d-predt) - o * np.maximum(0, predt-d))).reshape(y.size)\n",
    "              \n",
    "    def hessian(predt: np.ndarray, dtrain: xgb.DMatrix) -> np.ndarray:\n",
    "        return np.ones(predt.shape).reshape(predt.size)\n",
    "    \n",
    "    def custom_loss(predt: np.ndarray, dtrain: xgb.DMatrix) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        grad = gradient(predt, dtrain)\n",
    "        hess = hessian(predt, dtrain)\n",
    "        return grad, hess\n",
    "    \n",
    "    def newsvendorRMSE(predt: np.ndarray, dtrain: xgb.DMatrix) -> Tuple[str, float]:\n",
    "        y = dtrain.get_label().reshape(predt.shape)\n",
    "        d = y + np.matmul(np.maximum(0, y - predt), alpha_data)\n",
    "        v = np.sqrt(np.sum(np.power(d - predt, 2)))\n",
    "        return \"newsvendorRMSE\", v\n",
    "\n",
    "    X, y = X_train, target_train  \n",
    "    Xy = xgb.DMatrix(X, label=y)\n",
    "    results = {}\n",
    "    booster = xgb.train(\n",
    "        {\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"num_target\": target_train.shape[1],\n",
    "            \"multi_strategy\": \"multi_output_tree\",\n",
    "        },\n",
    "        dtrain=Xy,\n",
    "        num_boost_round=128,\n",
    "        obj=custom_loss,\n",
    "        evals=[(Xy, \"Train\")],\n",
    "        evals_result=results,\n",
    "        custom_metric=newsvendorRMSE\n",
    "       \n",
    "    )\n",
    "\n",
    "    preds = booster.predict(xgb.DMatrix(X_test, label=target_test))\n",
    "    print(\"Predictions:\", preds)\n",
    "    return preds\n",
    "\n",
    "target_pred_DT = custom_XGB_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall costs for ANN:  0\n",
      "Overall costs for DT:  62\n"
     ]
    }
   ],
   "source": [
    "# Loop over each week in target_test\n",
    "overall_costs_ANN = 0\n",
    "overall_costs_DT = 0\n",
    "\n",
    "for i in range(len(target_test)):\n",
    "    for j in range(len(target_test.columns)):\n",
    "        # Calculate understock and overstock costs\n",
    "        cost_ANN = 0\n",
    "        cost_DT = 0\n",
    "\n",
    "        #if target_pred_ANN.iloc[i, j] < target_test.iloc[i, j]:\n",
    "        #    cost_ANN = (prices[j] - costs[j]) * (target_test.iloc[i, j] - np.round(target_pred_ANN.iloc[i, j]))\n",
    "\n",
    "        #if target_pred_ANN.iloc[i, j] > target_test.iloc[i, j]:\n",
    "        #    cost_ANN = (costs[j] - salvages[j]) * (np.round(target_pred_ANN.iloc[i, j]) - target_test.iloc[i, j])\n",
    "        \n",
    "        if target_pred_DT[i, j] < target_test.iloc[i, j]:\n",
    "            cost_DT = (prices[j] - costs[j]) * (target_test.iloc[i, j] - np.round(target_pred_DT[i, j]))\n",
    "\n",
    "        if target_pred_DT[i, j] > target_test.iloc[i, j]:\n",
    "            cost_DT = (costs[j] - salvages[j]) * (np.round(target_pred_DT[i, j]) - target_test.iloc[i, j])\n",
    "        \n",
    "        # Calculate the total costs for the week\n",
    "        overall_costs_ANN += cost_ANN\n",
    "        overall_costs_DT += cost_DT\n",
    "\n",
    "# Print the overall costs\n",
    "print('Overall costs for ANN: ', int(overall_costs_ANN))\n",
    "print('Overall costs for DT: ', int(overall_costs_DT))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
